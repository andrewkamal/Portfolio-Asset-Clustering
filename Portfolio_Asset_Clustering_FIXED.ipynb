{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Asset Clustering Project - FIXED VERSION\n",
    "\n",
    "This notebook consolidates the working approach with proper preprocessing, PCA optimization, and clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Clustering Algorithms\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering, MiniBatchKMeans, Birch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Metrics and Validation\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique stocks: 165\n",
      "Date range: 2023-12-01 to 2025-11-28\n",
      "Trading days: 500\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_excel('NY165_stock_data.xlsx', sheet_name='Stock_Data')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Ticker', 'Date'])\n",
    "\n",
    "print(f\"Unique stocks: {df['Ticker'].nunique()}\")\n",
    "print(f\"Date range: {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
    "print(f\"Trading days: {df['Date'].nunique()}\")\n",
    "\n",
    "# Store metadata for later use\n",
    "meta_data = pd.DataFrame()\n",
    "if 'sector' in df.columns:\n",
    "    meta_data = df.groupby('Ticker')[['sector', 'industry', 'company']].first()\n",
    "    print(f\"\\nSectors: {df['sector'].nunique()}\")\n",
    "    print(df['sector'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating features for each stock...\n",
      "\n",
      "âœ“ Features calculated for 163 stocks\n",
      "âœ“ Total features: 11\n",
      "\n",
      "Feature list:\n",
      "  1. avg_volume\n",
      "  2. downside_vol\n",
      "  3. return_6m\n",
      "  4. return_mean\n",
      "  5. sharpe\n",
      "  6. skewness\n",
      "  7. sortino\n",
      "  8. var_95\n",
      "  9. vol_stability\n",
      "  10. volatility\n",
      "  11. volume_stability\n",
      "\n",
      "âœ“ Saved raw features to 01_raw_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "def calculate_advanced_features(group):\n",
    "    \"\"\"Calculate comprehensive features for clustering with consistent output structure.\"\"\"\n",
    "    \n",
    "    features = {}\n",
    "    returns = group['Daily_Return'].dropna()\n",
    "    prices = group['Close']\n",
    "    \n",
    "    # Check for sufficient data (at least 6 months)\n",
    "    if len(returns) < 126:\n",
    "        return pd.Series(features)\n",
    "    \n",
    "    # --- RETURNS ---\n",
    "    features['return_mean'] = returns.mean() * 252\n",
    "    features['return_6m'] = (prices.iloc[-1] / prices.iloc[-min(126, len(prices))] - 1) if len(prices) >= 126 else 0\n",
    "    \n",
    "    # --- VOLATILITY ---\n",
    "    features['volatility'] = returns.std() * np.sqrt(252)\n",
    "    features['downside_vol'] = returns[returns < 0].std() * np.sqrt(252) if (returns < 0).any() else 0\n",
    "    \n",
    "    vol_rolling = returns.rolling(20).std() * np.sqrt(252)\n",
    "    features['vol_stability'] = vol_rolling.std() / vol_rolling.mean() if vol_rolling.mean() > 0 else 0\n",
    "    \n",
    "    # --- RISK ADJUSTED ---\n",
    "    # Add small epsilon to avoid division by zero\n",
    "    features['sharpe'] = features['return_mean'] / (features['volatility'] + 1e-6)\n",
    "    features['sortino'] = features['return_mean'] / (features['downside_vol'] + 1e-6)\n",
    "    \n",
    "    # --- TAIL RISK & DIST ---\n",
    "    features['var_95'] = abs(returns.quantile(0.05))\n",
    "    features['skewness'] = returns.skew()\n",
    "    \n",
    "    # --- LIQUIDITY ---\n",
    "    if 'Volume' in group.columns:\n",
    "        volume = group['Volume']\n",
    "        # Use raw volume here; we log transform later in preprocessing\n",
    "        features['avg_volume'] = volume.mean() \n",
    "        features['volume_stability'] = volume.std() / (volume.mean() + 1e-6)\n",
    "    \n",
    "    return pd.Series(features)\n",
    "\n",
    "print(\"Calculating features for each stock...\")\n",
    "stock_features = df.groupby('Ticker').apply(calculate_advanced_features)\n",
    "\n",
    "# Handle MultiIndex Output if necessary\n",
    "if isinstance(stock_features, pd.Series) or isinstance(stock_features.index, pd.MultiIndex):\n",
    "    stock_features = stock_features.unstack()\n",
    "\n",
    "# Drop rows where all features are NaN (stocks with insufficient history)\n",
    "stock_features = stock_features.dropna(how='all')\n",
    "\n",
    "print(f\"\\nâœ“ Features calculated for {len(stock_features)} stocks\")\n",
    "print(f\"âœ“ Total features: {stock_features.shape[1]}\")\n",
    "print(f\"\\nFeature list:\")\n",
    "for i, col in enumerate(stock_features.columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "# Save raw features\n",
    "stock_features.to_excel('01_raw_features.xlsx', index=True)\n",
    "print(\"\\nâœ“ Saved raw features to 01_raw_features.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing & Optimization Pipeline\n",
    "\n",
    "This is the KEY section that makes clustering work well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPROCESSING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "[1] Initial stocks: 163 â†’ After dropping NaNs: 163\n",
      "\n",
      "[2] Clipping outliers (Winsorization 2.5%)...\n",
      "    â†’ Clipped 110 outlier values\n",
      "\n",
      "[3] Log-transforming skewed features...\n",
      "    â†’ Transformed: ['avg_volume', 'volatility', 'downside_vol', 'var_95']\n",
      "\n",
      "[4] Removing highly correlated features (>0.85)...\n",
      "    â†’ Dropping: ['sharpe', 'sortino', 'var_95', 'volatility']\n",
      "\n",
      "[5] Scaling features (RobustScaler)...\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING SUMMARY\n",
      "================================================================================\n",
      "Original features:        11\n",
      "Outliers clipped:         110\n",
      "Features after corr drop: 7\n",
      "Stocks in analysis:       163\n",
      "\n",
      "Final features: ['avg_volume', 'downside_vol', 'return_6m', 'return_mean', 'skewness', 'vol_stability', 'volume_stability']\n",
      "\n",
      "âœ“ Saved preprocessed features to 02_preprocessed_features.csv\n"
     ]
    }
   ],
   "source": [
    "def preprocess_and_optimize(df_features):\n",
    "    \"\"\"\n",
    "    Cleans, selects, and transforms features to maximize silhouette score.\n",
    "    This pipeline is CRITICAL for good clustering results!\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"PREPROCESSING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Drop NaN rows\n",
    "    df_clean = df_features.dropna()\n",
    "    print(f\"\\n[1] Initial stocks: {len(df_features)} â†’ After dropping NaNs: {len(df_clean)}\")\n",
    "    \n",
    "    if len(df_clean) < 30:\n",
    "        print(\"\\nâš  WARNING: Too few stocks after cleaning. Consider relaxing filters.\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Winsorization (Clip Outliers)\n",
    "    # Extreme outliers squash clusters. We clip top/bottom 2.5%\n",
    "    print(\"\\n[2] Clipping outliers (Winsorization 2.5%)...\")\n",
    "    outliers_clipped = 0\n",
    "    for col in df_clean.columns:\n",
    "        q025 = df_clean[col].quantile(0.025)\n",
    "        q975 = df_clean[col].quantile(0.975)\n",
    "        before = ((df_clean[col] < q025) | (df_clean[col] > q975)).sum()\n",
    "        df_clean[col] = winsorize(df_clean[col], limits=[0.025, 0.025])\n",
    "        outliers_clipped += before\n",
    "    print(f\"    â†’ Clipped {outliers_clipped} outlier values\")\n",
    "        \n",
    "    # 3. Log Transform Skewed Features\n",
    "    # Volatility and Volume are often log-normal. Transforming them helps KMeans.\n",
    "    print(\"\\n[3] Log-transforming skewed features...\")\n",
    "    skewed_cols = ['avg_volume', 'volatility', 'downside_vol', 'var_95']\n",
    "    transformed = []\n",
    "    for col in skewed_cols:\n",
    "        if col in df_clean.columns:\n",
    "            # Ensure positive before log\n",
    "            if (df_clean[col] <= 0).any():\n",
    "                df_clean[col] = np.log1p(df_clean[col] - df_clean[col].min() + 1e-6)\n",
    "            else:\n",
    "                df_clean[col] = np.log1p(df_clean[col])\n",
    "            transformed.append(col)\n",
    "    print(f\"    â†’ Transformed: {transformed}\")\n",
    "    \n",
    "    # 4. Feature Selection: Remove Highly Correlated Features\n",
    "    # High correlation adds weight to a specific dimension without adding info.\n",
    "    print(\"\\n[4] Removing highly correlated features (>0.85)...\")\n",
    "    corr_matrix = df_clean.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "    \n",
    "    if to_drop:\n",
    "        print(f\"    â†’ Dropping: {to_drop}\")\n",
    "        df_selected = df_clean.drop(columns=to_drop)\n",
    "    else:\n",
    "        print(f\"    â†’ No highly correlated features found\")\n",
    "        df_selected = df_clean\n",
    "    \n",
    "    # 5. Scaling\n",
    "    # RobustScaler is better than StandardScaler for financial data (less sensitive to outliers)\n",
    "    print(\"\\n[5] Scaling features (RobustScaler)...\")\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(df_selected)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=df_selected.columns, index=df_selected.index)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREPROCESSING SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Original features:        {df_features.shape[1]}\")\n",
    "    print(f\"Outliers clipped:         {outliers_clipped}\")\n",
    "    print(f\"Features after corr drop: {df_selected.shape[1]}\")\n",
    "    print(f\"Stocks in analysis:       {len(X_scaled)}\")\n",
    "    print(f\"\\nFinal features: {list(X_scaled.columns)}\")\n",
    "    \n",
    "    return X_scaled\n",
    "\n",
    "# Run preprocessing\n",
    "X_processed = preprocess_and_optimize(stock_features)\n",
    "\n",
    "if X_processed is not None:\n",
    "    X_processed.to_csv('02_preprocessed_features.csv')\n",
    "    print(\"\\nâœ“ Saved preprocessed features to 02_preprocessed_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PCA + K-Means Optimization (Grid Search)\n",
    "\n",
    "This jointly optimizes both PCA components AND number of clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZING CLUSTERS (PCA Components Ã— K)\n",
      "================================================================================\n",
      "\n",
      "Testing different combinations...\n",
      "\n",
      "\n",
      "Top 10 Combinations:\n",
      "==========================================================================================\n",
      "PCA   K    Silhouette   Balance    Calinski-H   Davies-B    \n",
      "==========================================================================================\n",
      "2     3    0.5845       0.042      239.84       0.7104      \n",
      "3     3    0.5255       0.043      165.44       0.8995      \n",
      "4     3    0.4964       0.043      132.11       1.0479      \n",
      "5     3    0.4659       0.043      116.04       1.1035      \n",
      "6     3    0.4509       0.043      106.96       1.1462      \n",
      "2     8    0.4132       0.030      264.64       0.6460      \n",
      "2     7    0.4037       0.029      250.69       0.6598      \n",
      "2     6    0.4012       0.088      246.30       0.7260      \n",
      "7     3    0.3957       0.044      101.65       1.1918      \n",
      "2     4    0.3655       0.080      251.25       0.8029      \n",
      "\n",
      "================================================================================\n",
      "ðŸ† OPTIMAL CONFIGURATION FOUND\n",
      "================================================================================\n",
      "Silhouette Score:     -1.0000\n",
      "Number of Clusters:   0\n",
      "PCA Components:       0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'explained_variance_ratio_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 91\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_processed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m     optimal_labels, X_pca_optimal, optimal_score, optimal_k, pca_model \u001b[38;5;241m=\u001b[39m find_best_clusters(X_processed)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     results_df \u001b[38;5;241m=\u001b[39m X_processed\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[1;32mIn[21], line 79\u001b[0m, in \u001b[0;36mfind_best_clusters\u001b[1;34m(X_scaled)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Clusters:   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA Components:       \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_pca\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariance Explained:   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_pca_model\u001b[38;5;241m.\u001b[39mexplained_variance_ratio_\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Cluster sizes\u001b[39;00m\n\u001b[0;32m     82\u001b[0m sizes \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(best_labels)\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39msort_index()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'explained_variance_ratio_'"
     ]
    }
   ],
   "source": [
    "def find_best_clusters(X_scaled):\n",
    "    \"\"\"\n",
    "    Grid search for best K and PCA components.\n",
    "    This is what makes the difference between 0.25 and 0.5 silhouette scores!\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OPTIMIZING CLUSTERS (PCA Components Ã— K)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nTesting different combinations...\\n\")\n",
    "    \n",
    "    best_score = -1\n",
    "    best_k = 0\n",
    "    best_pca = 0\n",
    "    best_labels = None\n",
    "    best_pca_data = None\n",
    "    best_pca_model = None\n",
    "    \n",
    "    results_log = []\n",
    "    \n",
    "    # Try different PCA components (2 to min(6, n_features))\n",
    "    n_features = X_scaled.shape[1]\n",
    "    pca_options = list(range(2, min(7, n_features) + 1))\n",
    "    \n",
    "    for n_components in pca_options:\n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        # Try different K (3 to 8)\n",
    "        # Avoid K=2 as it often just splits market vs non-market\n",
    "        for k in range(3, 9):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=20, max_iter=500)\n",
    "            labels = kmeans.fit_predict(X_pca)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            sil_score = silhouette_score(X_pca, labels)\n",
    "            cal_score = calinski_harabasz_score(X_pca, labels)\n",
    "            db_score = davies_bouldin_score(X_pca, labels)\n",
    "            \n",
    "            # Balance score\n",
    "            sizes = pd.Series(labels).value_counts().values\n",
    "            balance = sizes.min() / sizes.max()\n",
    "            \n",
    "            results_log.append({\n",
    "                'pca': n_components,\n",
    "                'k': k,\n",
    "                'silhouette': sil_score,\n",
    "                'calinski': cal_score,\n",
    "                'davies_bouldin': db_score,\n",
    "                'balance': balance\n",
    "            })\n",
    "            \n",
    "            # Update best if this is better\n",
    "            # We prioritize silhouette but also consider balance\n",
    "            if sil_score > best_score and balance > 0.3:  # Balance threshold\n",
    "                best_score = sil_score\n",
    "                best_k = k\n",
    "                best_pca = n_components\n",
    "                best_labels = labels\n",
    "                best_pca_data = X_pca\n",
    "                best_pca_model = pca\n",
    "    \n",
    "    # Display top 10 results\n",
    "    results_df = pd.DataFrame(results_log).sort_values('silhouette', ascending=False)\n",
    "    print(\"\\nTop 10 Combinations:\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"{'PCA':<5} {'K':<4} {'Silhouette':<12} {'Balance':<10} {'Calinski-H':<12} {'Davies-B':<12}\")\n",
    "    print(\"=\"*90)\n",
    "    for i, row in results_df.head(10).iterrows():\n",
    "        marker = \" â† SELECTED\" if (row['pca'] == best_pca and row['k'] == best_k) else \"\"\n",
    "        print(f\"{row['pca']:<5.0f} {row['k']:<4.0f} {row['silhouette']:<12.4f} {row['balance']:<10.3f} \"\n",
    "              f\"{row['calinski']:<12.2f} {row['davies_bouldin']:<12.4f}{marker}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ† OPTIMAL CONFIGURATION FOUND\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Silhouette Score:     {best_score:.4f}\")\n",
    "    print(f\"Number of Clusters:   {best_k}\")\n",
    "    print(f\"PCA Components:       {best_pca}\")\n",
    "    print(f\"Variance Explained:   {best_pca_model.explained_variance_ratio_.sum()*100:.2f}%\")\n",
    "    \n",
    "    # Cluster sizes\n",
    "    sizes = pd.Series(best_labels).value_counts().sort_index()\n",
    "    print(f\"\\nCluster Distribution:\")\n",
    "    for cluster, count in sizes.items():\n",
    "        print(f\"  Cluster {cluster}: {count:3d} stocks ({count/len(best_labels)*100:5.1f}%)\")\n",
    "    \n",
    "    return best_labels, best_pca_data, best_score, best_k, best_pca_model\n",
    "\n",
    "# Run optimization\n",
    "if X_processed is not None:\n",
    "    optimal_labels, X_pca_optimal, optimal_score, optimal_k, pca_model = find_best_clusters(X_processed)\n",
    "    \n",
    "    # Save results\n",
    "    results_df = X_processed.copy()\n",
    "    results_df['Cluster'] = optimal_labels\n",
    "    \n",
    "    # Add metadata if available\n",
    "    if not meta_data.empty:\n",
    "        results_df = results_df.join(meta_data)\n",
    "    \n",
    "    results_df.to_excel(f'03_optimized_clusters_k{optimal_k}_score{optimal_score:.3f}.xlsx')\n",
    "    print(f\"\\nâœ“ Saved results to 03_optimized_clusters_k{optimal_k}_score{optimal_score:.3f}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_processed is not None and optimal_labels is not None:\n",
    "    # Create 2D visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: PCA space (first 2 components)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, optimal_k))\n",
    "    \n",
    "    for cluster in range(optimal_k):\n",
    "        mask = optimal_labels == cluster\n",
    "        axes[0].scatter(X_pca_optimal[mask, 0], X_pca_optimal[mask, 1],\n",
    "                    c=[colors[cluster]], label=f'Cluster {cluster}',\n",
    "                    s=80, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        # Centroid\n",
    "        centroid = X_pca_optimal[mask].mean(axis=0)\n",
    "        axes[0].scatter(centroid[0], centroid[1], marker='X', s=400,\n",
    "                    c=[colors[cluster]], edgecolors='black', linewidth=2, zorder=10)\n",
    "    \n",
    "    var_ratio = pca_model.explained_variance_ratio_\n",
    "    axes[0].set_xlabel(f'PC1 ({var_ratio[0]*100:.1f}%)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel(f'PC2 ({var_ratio[1]*100:.1f}%)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title(f'Cluster Visualization - PCA Space\\n'\n",
    "                     f'Silhouette Score: {optimal_score:.3f}', fontsize=13, fontweight='bold')\n",
    "    axes[0].legend(loc='best', fontsize=9)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Feature importance (PCA loadings)\n",
    "    loadings = pca_model.components_[:2].T\n",
    "    feature_importance = np.abs(loadings).sum(axis=1)\n",
    "    feature_names = X_processed.columns\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "    \n",
    "    axes[1].barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')\n",
    "    axes[1].set_xlabel('Cumulative Absolute Loading (PC1 + PC2)', fontsize=11, fontweight='bold')\n",
    "    axes[1].set_title('Feature Importance in PCA', fontsize=13, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'04_visualization_k{optimal_k}_score{optimal_score:.3f}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ“ Visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cluster Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_processed is not None and optimal_labels is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CLUSTER PROFILING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Add cluster labels to original features\n",
    "    profile_df = stock_features.loc[X_processed.index].copy()\n",
    "    profile_df['Cluster'] = optimal_labels\n",
    "    \n",
    "    for cluster in sorted(profile_df['Cluster'].unique()):\n",
    "        cluster_data = profile_df[profile_df['Cluster'] == cluster]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CLUSTER {cluster} - {len(cluster_data)} stocks ({len(cluster_data)/len(profile_df)*100:.1f}%)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        numeric_cols = [col for col in profile_df.columns if col != 'Cluster']\n",
    "        cluster_means = cluster_data[numeric_cols].mean()\n",
    "        overall_means = profile_df[numeric_cols].mean()\n",
    "        \n",
    "        print(\"\\nKey Characteristics (vs overall average):\")\n",
    "        print(f\"{'Feature':<20} {'Cluster Avg':<15} {'Diff %':<12} {'Direction'}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            val = cluster_means[col]\n",
    "            overall = overall_means[col]\n",
    "            diff = ((val / overall - 1) * 100) if overall != 0 else 0\n",
    "            \n",
    "            if abs(diff) > 10:  # Only show significant differences\n",
    "                direction = 'â†‘â†‘ High' if diff > 25 else 'â†‘ Above' if diff > 10 else 'â†“ Below' if diff < -10 else 'â†“â†“ Low' if diff < -25 else 'â‰ˆ Average'\n",
    "                print(f\"{col:<20} {val:<15.4f} {diff:<12.1f} {direction}\")\n",
    "        \n",
    "        # Show sample stocks if we have company names\n",
    "        if not meta_data.empty and 'company' in meta_data.columns:\n",
    "            sample_stocks = cluster_data.join(meta_data['company']).head(5)\n",
    "            print(f\"\\nSample Stocks:\")\n",
    "            for ticker in sample_stocks.index:\n",
    "                company = sample_stocks.loc[ticker, 'company'] if 'company' in sample_stocks.columns else ticker\n",
    "                print(f\"  â€¢ {ticker}: {company}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Algorithm Comparison (Optional)\n",
    "\n",
    "Now let's see if other algorithms can beat K-Means!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_processed is not None and optimal_k is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARING CLUSTERING ALGORITHMS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nUsing optimal k={optimal_k} from K-Means optimization\\n\")\n",
    "    \n",
    "    clustering_results = {}\n",
    "    \n",
    "    # Use PCA data for fair comparison\n",
    "    X_for_comparison = X_pca_optimal\n",
    "    \n",
    "    # Define algorithms to test\n",
    "    algorithms = {\n",
    "        'K-Means (Optimized)': KMeans(n_clusters=optimal_k, random_state=42, n_init=20),\n",
    "        'Mini-Batch K-Means': MiniBatchKMeans(n_clusters=optimal_k, random_state=42, n_init=20),\n",
    "        'Hierarchical (Ward)': AgglomerativeClustering(n_clusters=optimal_k, linkage='ward'),\n",
    "        'Hierarchical (Average)': AgglomerativeClustering(n_clusters=optimal_k, linkage='average'),\n",
    "        'Gaussian Mixture': GaussianMixture(n_components=optimal_k, random_state=42, n_init=10),\n",
    "        'Spectral': SpectralClustering(n_clusters=optimal_k, random_state=42, affinity='nearest_neighbors'),\n",
    "    }\n",
    "    \n",
    "    for name, model in algorithms.items():\n",
    "        try:\n",
    "            labels = model.fit_predict(X_for_comparison)\n",
    "            \n",
    "            sil_score = silhouette_score(X_for_comparison, labels)\n",
    "            cal_score = calinski_harabasz_score(X_for_comparison, labels)\n",
    "            db_score = davies_bouldin_score(X_for_comparison, labels)\n",
    "            \n",
    "            sizes = pd.Series(labels).value_counts().values\n",
    "            balance = sizes.min() / sizes.max()\n",
    "            \n",
    "            clustering_results[name] = {\n",
    "                'labels': labels,\n",
    "                'silhouette': sil_score,\n",
    "                'calinski': cal_score,\n",
    "                'davies_bouldin': db_score,\n",
    "                'balance': balance\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš  {name} failed: {str(e)[:60]}\")\n",
    "            continue\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Algorithm': list(clustering_results.keys()),\n",
    "        'Silhouette': [v['silhouette'] for v in clustering_results.values()],\n",
    "        'Balance': [v['balance'] for v in clustering_results.values()],\n",
    "        'Calinski-H': [v['calinski'] for v in clustering_results.values()],\n",
    "        'Davies-B': [v['davies_bouldin'] for v in clustering_results.values()]\n",
    "    }).sort_values('Silhouette', ascending=False)\n",
    "    \n",
    "    print(\"\\nALGORITHM COMPARISON RESULTS\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Algorithm':<25} {'Silhouette':<15} {'Balance':<12} {'Calinski-H':<15} {'Davies-B':<12}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for idx, row in comparison_df.iterrows():\n",
    "        marker = \" â† BEST\" if idx == comparison_df.index[0] else \"\"\n",
    "        print(f\"{row['Algorithm']:<25} {row['Silhouette']:<15.4f} {row['Balance']:<12.3f} \"\n",
    "              f\"{row['Calinski-H']:<15.2f} {row['Davies-B']:<12.4f}{marker}\")\n",
    "    \n",
    "    best_algo = comparison_df.iloc[0]['Algorithm']\n",
    "    best_sil = comparison_df.iloc[0]['Silhouette']\n",
    "    \n",
    "    print(f\"\\nâœ“ Best Algorithm: {best_algo}\")\n",
    "    print(f\"âœ“ Silhouette Score: {best_sil:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_processed is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nâœ“ Successfully clustered {len(X_processed)} stocks\")\n",
    "    print(f\"âœ“ Achieved silhouette score: {optimal_score:.4f}\")\n",
    "    print(f\"âœ“ Number of clusters: {optimal_k}\")\n",
    "    print(f\"âœ“ PCA components used: {pca_model.n_components_}\")\n",
    "    print(f\"\\nðŸ“ Generated Files:\")\n",
    "    print(f\"   1. 01_raw_features.xlsx\")\n",
    "    print(f\"   2. 02_preprocessed_features.csv\")\n",
    "    print(f\"   3. 03_optimized_clusters_k{optimal_k}_score{optimal_score:.3f}.xlsx\")\n",
    "    print(f\"   4. 04_visualization_k{optimal_k}_score{optimal_score:.3f}.png\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Next Steps for Portfolio Optimization:\")\n",
    "    print(f\"   1. Select representative stocks from each cluster\")\n",
    "    print(f\"   2. Apply diversification constraints\")\n",
    "    print(f\"   3. Run mean-variance optimization\")\n",
    "    print(f\"   4. Backtest the portfolio\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"\\nâš  Clustering failed. Please check your data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
